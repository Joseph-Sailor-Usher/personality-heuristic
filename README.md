# What. Why. Ought.
The driving algorithm of volition is the question "what" meaning duck-typing and object recognizing in all of our precepts and memories (sensation, language, image, video, game) and "why" meaning the scientific method acting on all our configurations of "what"s in service of our preferences. "Why" can take us back into causal discovery, and also forwards into ethics and personality formation through algorithms similar to STRIPS, G.O.A.P., and the work of culture to share knowledge.

My intention here is to lay out a model which will only be improved on. In the simplest formation, I am saying that having "ought"s requires "what"s, "why"s. To algorithmically generate "ought"s, preferences, requires "what"s reality-division algorithms for object conception, recognition and simulation. It also requires "why"s causal discovery, ramification algorithms to apply hypotheses to generate predicted outcomes. The perception algorithms give us objects with attributes and classes. The explanation algorithms give us hypotheses and subjective story-telling. And the ethical algorithms take perception and theory and give us algebraic expressions of generalized tendencies of change in state variables over time and what that means in the larger context. Our "ought"s are what give us a personality heuristic in order to designate goals for states.

In a multiagent collaborative environment, agents capable of "ought" can guide the planning, similar to a creative director.

Let's focus on the "Why" layer. The problem of defining a planning domain definition language is that you choose how complex to make the description of hypotheses. A prototype has been built which uses atemporal, deterministic causal relationship which are one to many. It is one of the simplest descriptions of hypotheses. Causality happens immediately, there is one clear cause, and many effects. These are ideal parameters for causal discovery algorithms. I propose a meta-domain definition which allows interplay between causal domain definitions. Allow any type of hypotheses to be put into the ramification algorithm (STRIPS or G.O.A.P., etc) and when testing actions and hypotheses, check the attributes of the hypotheses and apply any algorithms capable of applying them to the world state. Since hypotheses of causality are applied to world states, not fundamental, we can then continue execution of ramification explorations of imagined futures, pasts, and stories.

I propose an algorithm capable of assigning attributes generally and formally to all combinations of perceivable information and concerned with finding the most useful attributes to focus attention. For example: (20 < battery < 80) encodes the "why"s "you might run out of battery", and "charging to nearly 100% will lower battery life" in one algebraic preference. Creating these preferences is similar to causal discovery in that you are creating the parameters of a domain of description language. Again, using the same adaptable domain definition praxis, "allow any type of preference to be put into the goal setting algorithm (most options open, tends to preferences, fewest options to enemies, most options to allies, etc) and when comparing experiences and actions to take at a given point in time or in possibility ramification for planning, check the preferences and apply any that have all their state variables present. Since perception of state variables are applied to world states, not fundamentally there, we can check for state variables to try to see if preferences will work even if they appear not to be applicable to a world state, then continue execution of assigning momentary preferences to available actions.

Since all of these preferences are in natural language and algebraic expressions tied to the "why"s that were used to create them, this entire algorithm is interpretable. These three areas of study will be the building blocks of AGI. 
Perception. Explanation. Preference.
What. Why. Ought.

# ChatGPT's version
# The Algorithm of Volition: What, Why, and Ought

## Summary

This document proposes a framework for Artificial Intelligence agents based on three core cognitive functions:

- **What** – Object recognition and conceptualization from experience.
- **Why** – Causal discovery and hypothesis testing via the scientific method.
- **Ought** – Preference formation through algebraic expressions of state variables, enabling goal setting and planning.

These processes together define a **volitional agent**: one capable of perception, explanation, and value-driven action. We call this emergent framework **Personality Heuristic Refinement**.

---

## The Core Algorithm

### 1. **What**: Perception and Classification

The “What” layer answers the fundamental question of identity through duck-typing and object recognition across modalities—language, image, sensation, memory. It parses experience into **objects with attributes and classes**, forming the basis for all further reasoning.

### 2. **Why**: Causal Discovery and Explanation

The “Why” layer acts as a recursive scientific method. It discovers causality, constructs hypotheses, and applies these to imagined or remembered scenarios. We encode causal relationships as **domain definitions**—flexible representations that can plug into planning systems (e.g. STRIPS, GOAP).

> A prototype causal system uses atemporal, deterministic rules where one cause leads to many effects. These simplified domains are ideal for computational exploration.

By treating hypotheses as testable patterns rather than fixed structures, we gain the ability to reason across diverse temporal and logical frameworks—past, future, imagined, and symbolic.

### 3. **Ought**: Preferences, Ethics, and Personality

The “Ought” layer expresses values as **algebraic preferences over state variables**. These are not hard-coded goals, but dynamic expressions shaped by context, perception, and causal understanding. For example:

```plaintext
(20 < battery < 80)
```
This single preference encodes both:

Avoid deep discharge (battery dying).
Avoid overcharging (battery degradation).
Preferences serve as heuristics guiding action selection, plan generation, and evaluation. When evaluating options, the agent selects actions aligned with interpretable, flexible expressions of its values.

## A Unified Framework for agents with personality heuristics

This tri-layered loop enables agents to:

Perceive, name, and model the world.
Form and refine hypotheses about causes and outcomes.
Express goals and values in structured, testable form.
In collaborative environments, such agents can serve as ethical guides, planners, and directors, shaping actions in alignment with collective intent.

This framework is open, interpretable, and extensible. It lays the groundwork for a new field:

Personality Heuristic Refinement Algorithms – The architecture of agentic minds.

